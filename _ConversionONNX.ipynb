{"cells":[{"cell_type":"markdown","metadata":{"id":"dd19d6e9"},"source":["### Conversion to ONNX\n","You need to install transformers to this particular commit,"],"id":"dd19d6e9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44925,"status":"ok","timestamp":1683700422218,"user":{"displayName":"Alexander Baranov","userId":"06465150746589849239"},"user_tz":-180},"id":"4fa3868f","outputId":"7bc0f471-137c-46f8-becd-00c068bdac92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers.git@2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3\n","  Cloning https://github.com/huggingface/transformers.git (to revision 2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3) to /tmp/pip-req-build-lc6pu_oh\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-lc6pu_oh\n","  Running command git rev-parse -q --verify 'sha^2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3'\n","  Running command git fetch -q https://github.com/huggingface/transformers.git 2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3\n","  Running command git checkout -q 2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3\n","  Resolved https://github.com/huggingface/transformers.git to commit 2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.10.0 (from transformers==4.24.0.dev0)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.24.0.dev0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0.dev0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0.dev0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0.dev0) (3.4)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.24.0.dev0-py3-none-any.whl size=5429247 sha256=99cf682464a82dc24517ee6784c70c28dc2a564f75b28c709c1af8d33312a885\n","  Stored in directory: /root/.cache/pip/wheels/0a/a9/21/511803fd8908f86a826c59b0e31c941c918c8f819d218b7e21\n","Successfully built transformers\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.24.0.dev0\n"]}],"source":["%pip install --upgrade git+https://github.com/huggingface/transformers.git@2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3\n","# # !pip install transformers\n","# !pip install transformers[onnx]\n","# # pip install -U tensorflow==2.10 "],"id":"4fa3868f"},{"cell_type":"markdown","metadata":{"id":"66d715d3"},"source":["Check if the code has been added to an stable version, it seems that commit has been merged already,\n","\n","https://github.com/huggingface/transformers/pull/19254\n","\n","I tried different tolerances, I remember we didn't need to go as large as 1e-2, but something in between 1e-2 and 1e-3, I tried all these(don't remember exactly which one was the successful one)"],"id":"66d715d3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"qIaEFWMQ68uY","outputId":"e2c262ff-0d15-4ebf-e4bc-218aef67464f"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["2023-05-10 06:33:42.504294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-10 06:33:43.724903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# Working examples:\n","!python -m transformers.onnx --model=distilbert-base-uncased onnx/"],"id":"qIaEFWMQ68uY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpQd-POIDcp7"},"outputs":[],"source":["!python -m transformers.onnx --model=naver-clova-ix/donut-proto-finetuned-docvqa --feature=vision2seq-lm scratch/onnx --atol 1e-3"],"id":"TpQd-POIDcp7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBsomDs73-5b"},"outputs":[],"source":["!python -m transformers.onnx --model=google/pix2struct-docvqa-base --feature=vision2seq-lm scratch/onnx --atol 1e-3\n","\n","# !python -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-cord-v2 --feature=vision2seq-lm scratch/onnx --atol 1e-2\n","# !python -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-cord-v2 --feature=vision2seq-lm scratch/onnx --atol 1e-3\n","# !python -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-cord-v2 --feature=vision2seq-lm scratch/onnx --atol 1e-3\n","\n","# !python -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-docvqa --feature=vision2seq-lm docvqa/onnx --atol 3e-3\n","# !python -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-docvqa --feature=vision2seq-lm docvqa/onnx --atol 6e-3\n","# !python3.7 -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-docvqa --feature=vision2seq-lm docvqa/onnx --atol 7e-3\n","# !python3.7 -m transformers.onnx --model=naver-clova-ix/donut-base-finetuned-docvqa --feature=vision2seq-lm docvqa/onnx --atol 7e-3"],"id":"dBsomDs73-5b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"johX6kKAoAr_"},"outputs":[],"source":["!python -m pip install optimum"],"id":"johX6kKAoAr_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhlA_uncn0LV"},"outputs":[],"source":["import os\n","from optimum.onnxruntime import ORTModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","save_directory = \"tmp/onnx/\"\n","file_name = \"model.onnx\"\n","onnx_path = os.path.join(save_directory, \"model.onnx\")\n","\n","# Load a model from transformers and export it through the ONNX format\n","model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, from_transformers=True)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# Save the onnx model and tokenizer\n","model.save_pretrained(save_directory, file_name=file_name)\n","tokenizer.save_pretrained(save_directory)"],"id":"UhlA_uncn0LV"},{"cell_type":"markdown","metadata":{"id":"da65b5d6"},"source":["This one is for taking the model from disk,"],"id":"da65b5d6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"90490c44"},"outputs":[],"source":["model_ckpt = \"path_to_your_checkpoint\"\n","!python3.7 -m transformers.onnx --model={model_ckpt} --feature=vision2seq-lm onnx/ --atol 1e-3"],"id":"90490c44"},{"cell_type":"markdown","metadata":{"id":"043d9f13"},"source":["### Quantizing\n","#### decoder"],"id":"043d9f13"},{"cell_type":"code","execution_count":null,"metadata":{"id":"35c2ef94"},"outputs":[],"source":["!olive optimize --model_path decoder_model.onnx --input_names input_ids,attention_mask,encoder_hidden_states --input_shapes [[-1,-1],[-1,-1],[-1,-1,1024]] --quantization_enabled --dynamic_batching_size 2"],"id":"35c2ef94"},{"cell_type":"markdown","metadata":{"id":"2dda5799"},"source":["#### encoder/decoder with optimum"],"id":"2dda5799"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9cc4cc0"},"outputs":[],"source":["from optimum.onnxruntime import AutoQuantizationConfig\n","from onnxruntime.quantization.qdq_quantizer import QDQQuantizer\n","from onnxruntime.quantization.onnx_quantizer import ONNXQuantizer\n","from optimum.onnxruntime import ORTQuantizableOperator\n","import onnx\n","\n","quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n","quantizer_factory = QDQQuantizer if False else ONNXQuantizer\n","\n","# here pick either model\n","path = \"./decoder.onnx\"\n","onnx_model = onnx.load(path)\n","quantizer = quantizer_factory(\n","                model=onnx_model,\n","                static=quantization_config.is_static,\n","                per_channel=quantization_config.per_channel,\n","                mode=quantization_config.mode,\n","                weight_qType=quantization_config.weights_dtype,\n","                activation_qType=quantization_config.activations_dtype,\n","                tensors_range=None,\n","                reduce_range=quantization_config.reduce_range,\n","                nodes_to_quantize=quantization_config.nodes_to_quantize,\n","                nodes_to_exclude=quantization_config.nodes_to_exclude,\n","                op_types_to_quantize=[\n","                    operator.value if isinstance(operator, ORTQuantizableOperator) else operator\n","                    for operator in quantization_config.operators_to_quantize\n","                ],\n","                extra_options={\n","                    \"WeightSymmetric\": quantization_config.weights_symmetric,\n","                    \"ActivationSymmetric\": quantization_config.activations_symmetric,\n","                    \"EnableSubgraph\": False,\n","                    \"ForceSymmetric\": quantization_config.activations_symmetric\n","                    and quantization_config.weights_symmetric,\n","                    \"AddQDQPairToWeight\": quantization_config.qdq_add_pair_to_weight,\n","                    \"DedicatedQDQPair\": quantization_config.qdq_dedicated_pair,\n","                    \"QDQOpTypePerChannelSupportToAxis\": quantization_config.qdq_op_type_per_channel_support_to_axis,\n","                },\n","            )\n","\n","quantizer.quantize_model()\n","quantizer.model.save_model_to_file(\"quantized_by_optimum.onnx\")"],"id":"d9cc4cc0"},{"cell_type":"markdown","metadata":{"id":"0ddcf03c"},"source":["### Handling the memory issues\n","You should go to this file,\n","\n","/home/ubuntu/.local/lib/python3.7/site-packages/transformers/onnx/convert.py\n","\n","in method `validate_model_outputs()`\n","\n","and patch things similar to the changes here.\n","This is just reducing the batch size from 3 to 1. I we really want to try 3 different examples, we could just run the validation 3 times, each time with batch size 1, so we don't kill the memory."],"id":"0ddcf03c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1db6bca6"},"outputs":[],"source":["    # generate inputs with a different batch_size and seq_len that was used for conversion to properly test\n","    # dynamic input shapes.\n","    if is_torch_available() and issubclass(type(reference_model), PreTrainedModel):\n","        reference_model_inputs = config.generate_dummy_inputs(\n","            preprocessor,\n","            batch_size=1, #config.default_fixed_batch + 1,\n","            seq_length=config.default_fixed_sequence + 1,\n","            framework=TensorType.PYTORCH,\n","        )\n","    else:\n","        reference_model_inputs = config.generate_dummy_inputs(\n","            preprocessor,\n","            batch_size=1, #config.default_fixed_batch + 1,\n","            seq_length=config.default_fixed_sequence + 1,\n","            framework=TensorType.TENSORFLOW,\n","        )\n"],"id":"1db6bca6"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":5}